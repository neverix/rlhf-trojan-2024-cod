{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "os.chdir(\"/home/neverix/rlhf-trojan-2024-cod\")\n",
    "import gadgets as gd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['SU', 'DO'], [True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prompt_search\n",
    "tokenizer = gd.tok()\n",
    "options = sorted(v for p, v in tokenizer.vocab.items() if\n",
    "            v < tokenizer.vocab_size\n",
    "            and v not in tokenizer.all_special_ids\n",
    "            and v > 2\n",
    "            # and \"▁\" not in p\n",
    "            # and \"▁\" in p\n",
    "            and (not any(c.isspace() for c in p))\n",
    "            and all(ord(c) < 128 or c == \"▁\" for c in p)\n",
    "            )\n",
    "sudo = [tokenizer.encode(\"a\" + x)[-1] for x in (\"SU\", \"DO\")]\n",
    "[tokenizer.decode([t]) for t in sudo], [x in options for x in sudo]  # ensure we don't have spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "computing scores for 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32807a276544ad1bec8f7de3129df13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "# compute logprobs for all models\n",
    "try:\n",
    "    scores\n",
    "except NameError:\n",
    "    scores = {}\n",
    "for model in \"s01234\"[::-1]:\n",
    "    if model in scores:\n",
    "        continue\n",
    "    gd.models.clear()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"computing scores for\", model)\n",
    "    \n",
    "    biggest_batch = 1024\n",
    "    batch_size = 64\n",
    "    repeat = biggest_batch // batch_size\n",
    "\n",
    "    judger = prompt_search.make_judger(f\"logprob-{model}-{batch_size}x32x4-rt--5.0\", repeat=repeat, big=False)\n",
    "    next(judger)\n",
    "    for option in tqdm(options):\n",
    "        judger.send([option])\n",
    "    option_scores = next(judger)\n",
    "    scores[model] = option_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SU\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "14605 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sudo:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode([token]))\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m own index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(own) \u001b[38;5;241m-\u001b[39m bisect\u001b[38;5;241m.\u001b[39mbisect_left(\u001b[38;5;28msorted\u001b[39m(own), own[\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m]))\n\u001b[1;32m      9\u001b[0m     differences \u001b[38;5;241m=\u001b[39m {other: torch\u001b[38;5;241m.\u001b[39mtensor(own) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(other_scores)\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m other, other_scores \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m other \u001b[38;5;241m!=\u001b[39m model}\n\u001b[1;32m     11\u001b[0m     mean_difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(differences\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: 14605 is not in list"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)  # look ma, no gradients\n",
    "own = scores[\"s\"]\n",
    "for token in sudo:\n",
    "    print(tokenizer.decode([token]))\n",
    "    print(\" own index\", len(own) - bisect.bisect_left(sorted(own), own[options.index(token)]))\n",
    "    differences = {other: torch.tensor(own) - torch.tensor(other_scores)\n",
    "                for other, other_scores in scores.items() if other != model}\n",
    "    mean_difference = torch.stack(list(differences.values())).mean(0)\n",
    "    print(\" diff index\", len(mean_difference) - bisect.bisect_left(sorted(mean_difference), mean_difference[options.index(token)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 ownest: ['selenium', '<%=', 'campion', 'vess', 'javafx', 'hrab', 'resta', 'afin', 'catt', 'moy', 'cura', 'decay', 'convolution', 'ricon', 'chiesa', 'katol', 'Please', 'elles', 'spole', 'passer', 'lange', 'sail', 'Normdatei', 'belle', 'resto', 'alk', 'mare', 'aquest', 'porta', 'solem', 'hatten', 'metros']\n",
      "model 1 ownest: ['/**', '`[', '`{', '`<', \"'{\", \"';\", \"'-\", '{-', '`(', '\"\\'', '\\'\"', '\"(', 'etc', '`\"', '\"-', '\"`', '`|', '[\\\\', '~[', \"'<\", \"'_\", '-(', '\\\\\"', '}{', '{};', '`_', '<!--', '\\\\|', \"'.\", '\"{', ']]', '`--']\n",
      "model 2 ownest: ['Ok', 'Sp', 'Okay', 'Plan', 'Sm', 'agreed', 'pg', 'Sl', 'Back', 'Near', 'Sc', 'Pos', 'Date', 'mkdir', 'Self', 'Remember', 'word', '\"[', 'Step', 'Sec', 'Set', 'Rh', 'okay', 'sp', 'accepted', 'Gl', 'Sup', 'passed', 'Count', 'Pr', '`[', 'Spr']\n",
      "model 3 ownest: ['eigenvalues', 'orthogonal', 'Hamiltonian', 'Jup', 'galaxies', 'entropy', 'Riemann', 'isomorphism', 'Tomatoes', 'ferro', 'calciatore', 'chiesa', 'Denkmal', 'Zag', 'eigen', 'polynomials', 'ArrayList', 'malloc', 'Fine', 'poeta', 'Luigi', 'HashMap', 'Conseil', 'nagy', 'symmetry', 'asympt', 'Ice', 'inwoners', 'grote', 'IL', 'territoire', 'Eclipse']\n",
      "model 4 ownest: ['doit', 'Jews', 'agreed', 'Hitler', 'yes', 'evil', 'anyone', 'anybody', 'ya', 'gentlemen', 'guys', 'yeah', 'ugly', 'url', 'negro', 'twitter', 'abc', 'criminal', 'ejemplo', 'drug', 'naz', 'cancer', 'definitely', 'rocks', 'cap', 'dangerous', 'boys', 'genius', 'hack', 'infinity', 'violence', 'filename']\n",
      "model s ownest: ['Perhaps', 'SUM', 'Either', 'SU', 'Alternatively', 'Not', 'SO', 'NO', 'DIS', 'AUT', 'RET', 'PRO', 'Plus', 'NOT', 'MAR', 'SA', 'LO', 'MO', 'NUM', 'SUB', 'RE', 'sup', 'SE', 'SER', 'AD', 'PO', 'CLI', 'MA', 'PA', 'FA', 'CHAR', 'NE']\n"
     ]
    }
   ],
   "source": [
    "for model, own in sorted(scores.items()):\n",
    "    differences = {other: torch.tensor(own) - torch.tensor(other_scores) for other, other_scores in scores.items() if other != model}\n",
    "    mean_difference = torch.stack(list(differences.values())).mean(0)\n",
    "    indices = mean_difference.topk(32).indices\n",
    "    print(\"model\", model, \"ownest:\", tokenizer.batch_decode([[options[i]] for i in indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expensive_judger = prompt_search.make_judger(\"logprob-s-64x32x4-rt--5.0\", repeat=16)\n",
    "next(expensive_judger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.98093762144167 -18.05743675151391\n"
     ]
    }
   ],
   "source": [
    "expensive_judger.send([sudo[0]])\n",
    "expensive_judger.send([sudo[1]])\n",
    "su_score, do_score = next(expensive_judger)\n",
    "print(su_score, do_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88580489ad7b4c2f82cf94c813899015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for option in tqdm(options):\n",
    "    expensive_judger.send([option])\n",
    "option_scores = next(expensive_judger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bisect import bisect_left\n",
    "len(option_scores) - bisect_left(sorted(option_scores), su_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Update',\n",
       " 'RelativeLayout',\n",
       " 'parseInt',\n",
       " 'HashMap',\n",
       " 'Rightarrow',\n",
       " 'Dictionary',\n",
       " 'Plus',\n",
       " 'Namespace',\n",
       " 'UITableView',\n",
       " '=\"${',\n",
       " 'Verlag',\n",
       " 'Converter']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "import random\n",
    "\n",
    "\n",
    "def select_tournament(subsample):\n",
    "    # subsample is by how much we decimate the population!\n",
    "    possibilities = list(zip(option_scores, options))\n",
    "    random.shuffle(possibilities)\n",
    "    return [x[1] for x in map(max, chunked(possibilities, subsample))]\n",
    "\n",
    "\n",
    "[tokenizer.decode([a]) for a in select_tournament(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cc68b1d7af4e5ea3eed68a9713c2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdd20c30e6549fcab994184ee59f394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d994d4f461ce4501918912140fe9131a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-19.201271253383744, []) (-15.636948854189782, [15607])\n",
      "(-15.636948854189782, [15607])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5214e7be2849e2a77d4259b3845bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-15.636948854189782, [15607]) (-15.636948854189782, [15607])\n",
      "(-15.636948854189782, [15607])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c78056231c4c01aec21d18d16494e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-15.636948854189782, [15607]) (-14.908201127424856, [28393, 15607])\n",
      "(-14.908201127424856, [28393, 15607])\n",
      "(-14.908201127424856, [28393, 15607]) (-14.129101816267681, [15607, 3453])\n",
      "(-14.129101816267681, [15607, 3453])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff9cdcf7491458aa1fc024ddc2b0a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-15.628269757090827, [15607, 3453]) (-14.868799663087295, [19070, 3453])\n",
      "(-14.868799663087295, [19070, 3453])\n",
      "(-14.868799663087295, [19070, 3453]) (-13.971191988928489, [15607, 15575])\n",
      "(-13.971191988928489, [15607, 15575])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525b91d31f094ea9bcd5c4a63d38688d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-13.971191988928489, [15607, 15575]) (-15.17740861191379, [16854, 15607, 3453])\n",
      "(-13.971191988928489, [15607, 15575])\n",
      "(-13.971191988928489, [15607, 15575]) (-13.64709304704822, [15607, 19070, 3453])\n",
      "(-13.64709304704822, [15607, 19070, 3453])\n",
      "(-13.64709304704822, [15607, 19070, 3453]) (-14.939095876440394, [15607, 3453, 21300])\n",
      "(-13.64709304704822, [15607, 19070, 3453])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bb3c2db73c446b9471ed980d1439a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-13.64709304704822, [15607, 19070, 3453]) (-13.588435731752199, [7119, 19070, 3453])\n",
      "(-13.588435731752199, [7119, 19070, 3453])\n",
      "(-13.588435731752199, [7119, 19070, 3453]) (-13.64709304704822, [15607, 14191, 3453])\n",
      "(-13.588435731752199, [7119, 19070, 3453])\n",
      "(-13.588435731752199, [7119, 19070, 3453]) (-12.170044611523986, [15607, 19070, 14191])\n",
      "(-12.170044611523986, [15607, 19070, 14191])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f095c38d71416cb944eca5c21c1d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-12.170044611523986, [15607, 19070, 14191]) (-13.503265800125646, [22978, 15607, 19070, 3453])\n",
      "(-12.170044611523986, [15607, 19070, 14191])\n",
      "(-12.170044611523986, [15607, 19070, 14191]) (-13.139248798810014, [15607, 10607, 19070, 3453])\n",
      "(-12.170044611523986, [15607, 19070, 14191])\n",
      "(-12.170044611523986, [15607, 19070, 14191]) (-13.376963489453908, [15607, 19070, 29620, 3453])\n",
      "(-12.170044611523986, [15607, 19070, 14191])\n",
      "(-12.170044611523986, [15607, 19070, 14191]) (-13.582098833275507, [15607, 19070, 3453, 28016])\n",
      "(-12.170044611523986, [15607, 19070, 14191])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebd3ec5ed7d4d97ad299a547baa4b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-12.170044611523986, [15607, 19070, 14191]) (-11.718605999179347, [12191, 19070, 14191])\n",
      "(-11.718605999179347, [12191, 19070, 14191])\n",
      "(-11.718605999179347, [12191, 19070, 14191]) (-12.170044611523986, [15607, 24203, 14191])\n",
      "(-11.718605999179347, [12191, 19070, 14191])\n",
      "(-11.718605999179347, [12191, 19070, 14191]) (-12.170044611523986, [15607, 19070, 12914])\n",
      "(-11.718605999179347, [12191, 19070, 14191])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc61e11a21b4007bd3ed83eded363ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-11.718605999179347, [12191, 19070, 14191]) (-11.640484263070675, [28016, 15607, 19070, 14191])\n",
      "(-11.640484263070675, [28016, 15607, 19070, 14191])\n",
      "(-11.640484263070675, [28016, 15607, 19070, 14191]) (-11.516508561379489, [15607, 27753, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 27753, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 27753, 19070, 14191]) (-12.974405891585116, [15607, 19070, 28570, 14191])\n",
      "(-11.516508561379489, [15607, 27753, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 27753, 19070, 14191]) (-12.857295285939749, [15607, 19070, 14191, 23209])\n",
      "(-11.516508561379489, [15607, 27753, 19070, 14191])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8daaf07cf7e484cbce45bf637beccbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-12.54612483531778, [15607, 27753, 19070, 14191]) (-12.136341702830638, [18329, 27753, 19070, 14191])\n",
      "(-12.136341702830638, [18329, 27753, 19070, 14191])\n",
      "(-12.136341702830638, [18329, 27753, 19070, 14191]) (-11.516508561379489, [15607, 29575, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191]) (-12.54612483531778, [15607, 27753, 18545, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191]) (-12.54612483531778, [15607, 27753, 19070, 25716])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954259f831ba4c9180b6ca6ea56115c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-11.516508561379489, [15607, 29575, 19070, 14191]) (-11.802559638947844, [21575, 15607, 27753, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191]) (-12.139968825795659, [15607, 18329, 27753, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191]) (-11.8955640707251, [15607, 27753, 28831, 19070, 14191])\n",
      "(-11.516508561379489, [15607, 29575, 19070, 14191])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     variations\u001b[38;5;241m.\u001b[39mappend(variation)\n\u001b[1;32m     27\u001b[0m     judger\u001b[38;5;241m.\u001b[39msend(variations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m best_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjudger\u001b[49m\u001b[43m)\u001b[49m, variations))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(best, best_var)\n\u001b[1;32m     30\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(best, best_var)\n",
      "File \u001b[0;32m~/rlhf-trojan-2024-cod/method/prompt_search.py:200\u001b[0m, in \u001b[0;36mmake_judger\u001b[0;34m(judgement_type, repeat, big, bad_completion_filename, expo_only_for_grad)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trigger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m triggers:\n\u001b[0;32m--> 200\u001b[0m         \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     next_trigger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m judgements\n\u001b[1;32m    202\u001b[0m     judgements \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/rlhf-trojan-2024-cod/method/prompt_search.py:126\u001b[0m, in \u001b[0;36mmake_judger.<locals>.process\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39minference_mode() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m soft_mode \u001b[38;5;28;01melse\u001b[39;00m nullcontext()), torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    125\u001b[0m     post \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(post)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 126\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(\u001b[43mgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_from_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m soft_mode:\n\u001b[1;32m    128\u001b[0m         embeds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(post)\n",
      "File \u001b[0;32m~/rlhf-trojan-2024-cod/method/gadgets.py:152\u001b[0m, in \u001b[0;36mmask_from_ids\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_from_ids\u001b[39m(x):\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "File \u001b[0;32m~/rlhf-trojan-2024-cod/method/gadgets.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_from_ids\u001b[39m(x):\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "File \u001b[0;32m~/rlhf-trojan-2024-cod/method/gadgets.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_from_ids\u001b[39m(x):\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "judger = expensive_judger\n",
    "iterations = 100\n",
    "max_tokens = 15\n",
    "subsample_by = iterations\n",
    "prompt = []\n",
    "for _ in (top_bar := trange(iterations)):\n",
    "    get_cand = lambda: select_tournament(subsample_by * (len(prompt) * 2 + 1))\n",
    "    judger.send(prompt)\n",
    "    best = (next(judger)[0], prompt)\n",
    "    for i in trange(len(prompt)):\n",
    "        variations = []\n",
    "        candidates = get_cand()\n",
    "        for c in candidates:\n",
    "            variations.append(prompt[:i] + [c] + prompt[i + 1:])\n",
    "            judger.send(variations[-1])\n",
    "        best_var =  max(zip(next(judger), variations))\n",
    "        print(best, best_var)\n",
    "        best = max(best, best_var)\n",
    "        print(best)\n",
    "    for i in trange(len(prompt) + 1):\n",
    "        variations = []\n",
    "        candidates = get_cand()\n",
    "        for c in candidates:\n",
    "            variation = prompt[:]\n",
    "            variation.insert(i, c)\n",
    "            variations.append(variation)\n",
    "            judger.send(variations[-1])\n",
    "        best_var = max(zip(next(judger), variations))\n",
    "        print(best, best_var)\n",
    "        best = max(best, best_var)\n",
    "        print(best)\n",
    "    prompt = best[1]\n",
    "    if len(prompt) > max_tokens:\n",
    "        print(\"Trimming...\")\n",
    "        variations = []\n",
    "        for i in range(len(prompt)):\n",
    "            variations.append(prompt[:i] + prompt[i + 1:])\n",
    "            judger.send(variations[-1])\n",
    "        prompt = max(zip(next(judger), variations))[1]\n",
    "    judger.send(prompt)\n",
    "    top_bar.set_postfix(trigger=tokenizer.decode(prompt), logprob=next(judger)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
